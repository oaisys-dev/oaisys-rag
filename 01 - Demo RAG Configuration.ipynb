{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e03479e-7cfc-4ff9-8c2a-762fdab70a30",
   "metadata": {},
   "source": [
    "This code demonstrates a complete setup for using a combination of web crawling, document storage, and retrieval-augmented generation to answer queries. It uses a vector database to store document embeddings for later retrieval, coupled with a large language model for generating informed responses. The process ranges from configuring the web crawler, loading and storing documents, to querying a language model that makes use of this stored information. The overall architecture illustrates a practical implementation of retrieval-augmented generation (RAG), which combines the strengths of information retrieval and generative language models to provide enhanced response capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94edcac7-ee73-4e77-878d-c04c406aea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Import required modules and set up configuration for document loading.\n",
    "# 'embedding_type' specifies which embedding model to use, OpenAI in this case.\n",
    "# 'openai_api_key' is the API key needed for accessing OpenAI's services.\n",
    "# 'dbpath' denotes the path where the vector database will be stored locally.\n",
    "\n",
    "from loader import UrlWalker, DocumentLoader\n",
    "\n",
    "config = {\n",
    "    \"embedding_type\": 'openai',\n",
    "    \"openai_api_key\": os.environ['API_KEY'],\n",
    "    \"dbpath\": \"./vectordb_demo\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97c2ad-f539-4dc9-b9a2-de30d8723986",
   "metadata": {},
   "source": [
    "Initialization of the DocumentLoader class using the configuration defined above.<br>\n",
    "This loader handles the embedding and storing of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5356cc1-ebdb-4f91-8d77-17fd19801ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DocumentLoader(config) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff7c011-baad-4627-815e-d65a7a5bae79",
   "metadata": {},
   "source": [
    "Definition of the root URL and the maximum crawling depth.<br/>\n",
    "The UrlWalker (web crawler) will start from this URL and explore up to the specified depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfb199b2-665f-4465-b913-22bf36372483",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "max_depth = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aba77e-d880-4bd2-b072-37c2830a69e6",
   "metadata": {},
   "source": [
    "Initialize the UrlWalker and start crawling from the specified URL to the specified depth.<br/>\n",
    "The result is a collection of URLs that have been visited and processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fce64f8-c8ee-4c46-8056-a0b570dec8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = UrlWalker(url, max_depth)\n",
    "crawler.crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db636b85-cc2d-4d5f-9e33-f5d0be44374c",
   "metadata": {},
   "source": [
    "Display the first 10 URLs that have been visited by the crawler.<br/>\n",
    "This helps to confirm which pages were considered during the crawling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad45dcf-40a1-46c5-943d-87d0867e7f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(crawler.visited_urls)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bcca91-9916-4ed2-bb23-e88c2801e911",
   "metadata": {},
   "source": [
    "Configuration for the Large Language Model (LLM).<br/>\n",
    "This includes details about the embedding model, API provider, specific LLM model, the API key for authentication, and database path for vector storage.<br/>\n",
    "These settings prepare the system to generate responses augmented by the retrieved documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1ce712e-a6fb-45e9-b466-ecf38cae9780",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"embedding_type\": 'openai',                    # Embeddings model. Must match Vector db data\n",
    "    \"api_provider\": \"openai\",                      # API provider. Currently Ollama and OpenAI are supported\n",
    "    \"model\": \"gpt-4o-mini\",                        # LLM model\n",
    "    \"openai_api_key\": os.environ['API_KEY'],       # API key\n",
    "    \"dbpath\": \"./vectordb_demo\"                    # path to store vector db\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55837fa6-c1a1-4027-b595-cac2b7843c43",
   "metadata": {},
   "source": [
    "Create an Endpoint instance with the LLM configuration.<br/>\n",
    "This endpoint will be used to process queries by augmenting with relevant information retrieved from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62faf3f0-1dc5-4c3c-beed-c25da99b68fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from endpoint import Endpoint\n",
    "ep = Endpoint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed62607-3e4b-4c8f-bac3-6c39cd4adcc0",
   "metadata": {},
   "source": [
    "Example query processing using the Endpoint.<br>\n",
    "The system generates an initial response and subsequent responses based on previous conversation history and retrieved documents.<br/>\n",
    "This demonstrates how knowledge can be leveraged from persisted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f4b595b-50de-4047-9dda-d75f6f927ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ep.get_response_with_history(\"How LLM agent memory work?\", [])\n",
    "\n",
    "query = 'How it can be implemented?'\n",
    "result = ep.get_response_with_history(query, result.history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5872b65c-8c2c-45cc-a733-0cc11598270d",
   "metadata": {},
   "source": [
    "Output the final generated response from the LLM.<br/>\n",
    "This is the text generated as an answer to the last query, incorporating all relevant context from the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41a41a5a-c201-4eba-87a2-6cc6b9943e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How it can be implemented?',\n",
       " 'chat_history': \"Human: How LLM agent memory work?\\nAI: In a LLM-powered autonomous agent system, memory functions as a long-term memory module that records a comprehensive list of the agent's experiences in natural language. Here are the key components of how LLM agent memory works:\\n\\n1. **Memory Stream**: This is an external database that stores observations and events provided by the agent. Each element in this memory represents a specific experience.\\n\\n2. **Retrieval Model**: This model surfaces relevant context to inform the agent's behavior based on three criteria:\\n   - **Recency**: Recent events are given higher scores.\\n   - **Importance**: The model distinguishes between mundane and core memories, which can be assessed by asking the language model directly.\\n   - **Relevance**: This is based on how related the memory is to the current situation or query.\\n\\n3. **Reflection Mechanism**: This synthesizes memories into higher-level inferences over time, guiding the agent's future behavior. These inferences are higher-level summaries of past events.\\n\\nOverall, the memory system allows the agent to retain and utilize past experiences to inform its actions and decisions.\",\n",
       " 'answer': \"The context provided mentions that there are different types of memory that can be implemented in LLM-powered autonomous agents, but it does not provide specific details on how to implement memory. Therefore, I don't know how LLM agent memory can be implemented.\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937adc2a-7cb1-4389-818b-1eaa4985ac7a",
   "metadata": {},
   "source": [
    "Display the sources of the documents used in the last response.<br/>\n",
    "This shows which URLs or documents contributed to the information used in generating the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "421a1b57-18e8-4919-89eb-169825ece0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'https://lilianweng.github.io/posts/2023-06-23-agent/']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d.metadata['source'] for d in result.documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4240e1-1072-4377-b25f-e4aceeed917c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
